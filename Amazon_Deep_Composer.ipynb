{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "metadata": {
          "collapsed": false
        },
        "source": []
      }
    },
    "colab": {
      "name": "Amazon-Deep-Composer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asigalov61/Amazon-Deep-Composer/blob/master/Amazon_Deep_Composer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQKt8c_nv5RB",
        "colab_type": "text"
      },
      "source": [
        "# Training a custom AR-CNN model \n",
        "In this Jupyter notebook, we guide you through several steps of the data science life cycle. We explain how to acquire the data that you use for this project, \n",
        "provide some exploratory data analysis (**EDA**), and show how we augment the data during training. \n",
        "\n",
        "\n",
        "### The AWS DeepComposer approach to generating music  \n",
        "Autoregressive-based approaches are prone to accumulate errors during training. To help mitigate this problem, we train our AR-CNN model so that it can detect and then fix mistakes, including those made by the model itself.\n",
        "\n",
        "We do this by treating music generation as a series of *edit events*, which can be either the addition or removal of a note. An *edit sequence* is a series of edit events. Every edit sequence can directly correspond to a piano roll.\n",
        "\n",
        "By training our model to view the problem as edit events rather than as an entire image or just the addition of notes, we found that it can offset the accumulation of errors and generate higher quality music.\n",
        "\n",
        "Now that you understand the basic theory behind our approach, let’s dive into the code. In the next section, we show examples of the piano roll format that we use for training the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Mngh-Xnv5RC",
        "colab_type": "text"
      },
      "source": [
        "## Installing dependencies\n",
        "First, let's install and import all of the Python packages that we will use in this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "51DsSASCv5RD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The MIT-Zero License\n",
        "\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "# of this software and associated documentation files (the \"Software\"), to deal\n",
        "# in the Software without restriction, including without limitation the rights\n",
        "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "# copies of the Software, and to permit persons to whom the Software is\n",
        "# furnished to do so.\n",
        "\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
        "# THE SOFTWARE.\n",
        "\n",
        "\n",
        "# Create the environment and install required packages\n",
        "!git clone https://github.com/asigalov61/aws-deepcomposer-samples.git\n",
        "%cd /content/aws-deepcomposer-samples/ar-cnn\n",
        "%tensorflow_version 1.x\n",
        "!pip install -r requirements.txt\n",
        "!pip install tensorflow-gpu==1.15.3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false,
          "name": "#%%\n"
        },
        "id": "oE1KlxnZv5RH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports\n",
        "import os\n",
        "import glob\n",
        "import json\n",
        "import numpy as np\n",
        "import keras\n",
        "from enum import Enum\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, BatchNormalization, Dropout\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras import backend as K\n",
        "from random import randrange\n",
        "import random\n",
        "import math\n",
        "import pypianoroll\n",
        "from utils.midi_utils import play_midi, plot_pianoroll, get_music_metrics, process_pianoroll, process_midi\n",
        "from constants import Constants\n",
        "from augmentation import AddAndRemoveAPercentageOfNotes\n",
        "from data_generator import PianoRollGenerator\n",
        "from utils.generate_training_plots import GenerateTrainingPlots\n",
        "from inference import Inference\n",
        "from model import OptimizerType\n",
        "from model import ArCnnModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmf4AI3Pv5RK",
        "colab_type": "text"
      },
      "source": [
        "## Importing the data \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DA-JvWGv5RK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!unzip data/JSB\\ Chorales.zip -d data\n",
        "#@title (Best Choice/Works best stand-alone) Alex Piano Only Drafts Original 1500 MIDIs \n",
        "%cd /content/aws-deepcomposer-samples/ar-cnn/input\n",
        "!wget 'https://github.com/asigalov61/AlexMIDIDataSet/raw/master/AlexMIDIDataSet-CC-BY-NC-SA-All-Drafts-Piano-Only.zip'\n",
        "!unzip -j 'AlexMIDIDataSet-CC-BY-NC-SA-All-Drafts-Piano-Only.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ub7GoYnv5RN",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "is_executing": false
        },
        "id": "Lzj8VkNLv5RN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Import the MIDI files from the data_dir and save them with the midi_files variable  \n",
        "data_dir = '/content/aws-deepcomposer-samples/ar-cnn/input/*.mid'\n",
        "midi_files = glob.glob(data_dir)\n",
        "\n",
        "#Finds our random MIDI file from the midi_files variable and then plays it\n",
        "#Note: To listen to multiple samples from the Bach dataset, you can run this cell over and over again. \n",
        "random_midi = randrange(len(midi_files))\n",
        "play_midi(midi_files[random_midi])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8wkjqT4v5RR",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing the data into the *piano roll* format\n",
        "\n",
        "### Reviewing sample piano rolls \n",
        "\n",
        "### Why do we use 128 timesteps?\n",
        "In this tutorial, we use 8-[bar](https://en.wikipedia.org/wiki/Bar_(music)) samples from the dataset. We subdivide those 8 bars into 128 timesteps. That's because each of the 8 bars contains 4 beats. We further divide each beat into 4 timesteps. \n",
        "\n",
        "This yields 128 timesteps:\n",
        "\n",
        "$$ \\frac{4\\;timesteps}{1\\;beat} * \\frac{4\\;beats}{1\\;bar} * \\frac{8\\;bars}{1} = 128\\;timesteps $$\n",
        "\n",
        "We found that this level of resolution is sufficient to capture the musical details in our dataset.\n",
        "\n",
        "### Creating samples of uniform size (shape) for model training \n",
        "\n",
        "For model training, the *input piano rolls* must be the same size. As you saw when we used the `play_midi` function, each sample isn't the same length. We use two functions to create *target piano rolls* that are the same size: `process_midi` and `process_pianoroll`. These functions are wrapped in a larger function, `generate_samples`, which also takes in constants that are related to subdividing the .mid files.\n",
        "\n",
        "#### In the code cells below:\n",
        "- `generate_samples` is a function used to ingest the midi files and break the files down into a uniform shape\n",
        "- `plot_pianoroll` uses a built in function `plot_track` from the  [`pypianoroll`](https://salu133445.github.io/pypianoroll/visualization.html) library to plot a piano roll track from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckZvLi3Zv5RS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate MIDI file samples\n",
        "def generate_samples(midi_files, bars, beats_per_bar, beat_resolution, bars_shifted_per_sample):\n",
        "    \"\"\"\n",
        "    dataset_files: All files in the dataset\n",
        "    return: piano roll samples sized to X bars\n",
        "    \"\"\"\n",
        "    timesteps_per_nbars = bars * beats_per_bar * beat_resolution\n",
        "    time_steps_shifted_per_sample = bars_shifted_per_sample * beats_per_bar * beat_resolution\n",
        "    samples = []\n",
        "    for midi_file in midi_files:\n",
        "        pianoroll = process_midi(midi_file, beat_resolution) # Parse the MIDI file and get the piano roll\n",
        "        samples.extend(process_pianoroll(pianoroll, time_steps_shifted_per_sample, timesteps_per_nbars))\n",
        "    return samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6UUobIrpv5RV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Saving the generated samples into a dataset variable \n",
        "dataset_samples = generate_samples(midi_files, Constants.bars, Constants.beats_per_bar,Constants.beat_resolution, Constants.bars_shifted_per_sample)\n",
        "# Shuffle the dataset\n",
        "random.shuffle(dataset_samples);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bVKmxgyv5RX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualize a random piano roll from the dataset \n",
        "random_pianoroll = dataset_samples[randrange(len(dataset_samples))]\n",
        "plot_pianoroll(pianoroll = random_pianoroll,\n",
        "               beat_resolution = 4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzGQH_T0v5Ra",
        "colab_type": "text"
      },
      "source": [
        "## Augmenting data during training\n",
        "\n",
        "### Adding or removing notes during training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMb9BIFXv5Rb",
        "colab_type": "text"
      },
      "source": [
        "### Removing random notes from a target piano roll to create input piano rolls\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQWNJtkFv5Rb",
        "colab_type": "text"
      },
      "source": [
        "### Adding random notes to the target piano roll to create input piano rolls \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wec5hOVNv5Rc",
        "colab_type": "text"
      },
      "source": [
        "### Sampling from a uniform distribution\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLHaOrByv5Rc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampling_lower_bound_remove = 0 \n",
        "sampling_upper_bound_remove = 100\n",
        "sampling_lower_bound_add = 1\n",
        "sampling_upper_bound_add = 1.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKVxuXjyv5Rf",
        "colab_type": "text"
      },
      "source": [
        "## Calculating the loss function\n",
        "\n",
        "\n",
        ">**NOTE**: The model can pick any of the notes that were added or removed during data augmentation.\n",
        "\n",
        "This difference in distributions between the distribution representing the *input piano roll* and the uniform distribution can be calculated as the *Kullback–Leibler divergence*. Our loss function is the difference between the model’s output and the uniform distribution of all of the pixel (note) probabilities in the symmetric difference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXOaegA7v5Rg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Customized loss function\n",
        "class Loss():\n",
        "    @staticmethod \n",
        "    def built_in_softmax_kl_loss(target, output):\n",
        "        '''\n",
        "        Custom Loss Function\n",
        "        :param target: ground truth values\n",
        "        :param output: predicted values\n",
        "        :return kullback_leibler_divergence loss\n",
        "        '''\n",
        "        target = K.flatten(target)\n",
        "        output = K.flatten(output)\n",
        "        target = target / K.sum(target)\n",
        "        output = K.softmax(output)\n",
        "        return keras.losses.kullback_leibler_divergence(target, output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vncd22kbv5Ri",
        "colab_type": "text"
      },
      "source": [
        "## Model architecture\n",
        "\n",
        "### Creating the model architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfiZ5Gojv5Ri",
        "colab_type": "text"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XAQY-2yLv5SZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_size = len(dataset_samples)\n",
        "dataset_split = math.floor(dataset_size * Constants.training_validation_split) \n",
        "\n",
        "training_samples = dataset_samples[0:dataset_split]\n",
        "print(\"training samples length: {}\".format(len(training_samples)))\n",
        "validation_samples = dataset_samples[dataset_split + 1:dataset_size]\n",
        "print(\"validation samples length: {}\".format(len(validation_samples)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wQGiYqKv5Sc",
        "colab_type": "text"
      },
      "source": [
        "### Specifying training hyperparameters \n",
        "\n",
        ">**NOTE** \n",
        ">If you want to test that your model is training on your custom dataset, you can decrease the number of `epochs` down to **1** in the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQamt3I6v5Sd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Piano Roll Input Dimensions\n",
        "input_dim = (Constants.bars * Constants.beats_per_bar * Constants.beat_resolution, \n",
        "             Constants.number_of_pitches, \n",
        "             Constants.number_of_channels)\n",
        "# Number of Filters In The Convolution\n",
        "num_filters = 32\n",
        "# Growth Rate Of Number Of Filters At Each Convolution\n",
        "growth_factor = 2\n",
        "# Number Of Encoder And Decoder Layers\n",
        "num_layers = 5\n",
        "# A List Of Dropout Values At Each Encoder Layer\n",
        "dropout_rate_encoder = [0, 0.5, 0.5, 0.5, 0.5]\n",
        "# A List Of Dropout Values At Each Decoder Layer\n",
        "dropout_rate_decoder = [0.5, 0.5, 0.5, 0.5, 0]\n",
        "# A List Of Flags To Ensure If batch_normalization Should be performed At Each Encoder\n",
        "batch_norm_encoder = [True, True, True, True, False]\n",
        "# A List Of Flags To Ensure If batch_normalization Should be performed At Each Decoder\n",
        "batch_norm_decoder = [True, True, True, True, False]\n",
        "# Path to Pretrained Model If You Want To Initialize Weights Of The Network With The Pretrained Model\n",
        "pre_trained = False\n",
        "# Learning Rate Of The Model\n",
        "learning_rate = 0.001\n",
        "# Optimizer To Use While Training The Model\n",
        "optimizer_enum = OptimizerType.ADAM\n",
        "# Batch Size\n",
        "batch_size = 128\n",
        "# Number Of Epochs\n",
        "epochs = 500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7qqyXuyv5Sg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The Number of Batch Iterations Before A Training Epoch Is Considered Finished\n",
        "steps_per_epoch = int(\n",
        "    len(training_samples) * Constants.samples_per_ground_truth_data_item / int(batch_size))\n",
        "\n",
        "print(\"The Total Number Of Steps Per Epoch Are: \"+ str(steps_per_epoch))\n",
        "\n",
        "# Total Number Of Time Steps\n",
        "n_timesteps = Constants.bars * Constants.beat_resolution * Constants.beats_per_bar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPfwQws7v5Si",
        "colab_type": "text"
      },
      "source": [
        "### Creating the data generators that perform data augmentation\n",
        "\n",
        "To create the *input piano rolls* during training, we need data generators for both the training and validation samples. For our purposes, we use a custom data generator to perform data augmentation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6zlQvcUv5Sj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Training Data Generator\n",
        "training_data_generator = PianoRollGenerator(sample_list=training_samples,\n",
        "                                             sampling_lower_bound_remove = sampling_lower_bound_remove,\n",
        "                                             sampling_upper_bound_remove = sampling_upper_bound_remove,\n",
        "                                             sampling_lower_bound_add = sampling_lower_bound_add,\n",
        "                                             sampling_upper_bound_add = sampling_upper_bound_add,\n",
        "                                             batch_size = batch_size,\n",
        "                                             bars = Constants.bars,\n",
        "                                             samples_per_data_item = Constants.samples_per_ground_truth_data_item,\n",
        "                                             beat_resolution = Constants.beat_resolution,\n",
        "                                             beats_per_bar = Constants.beats_per_bar,\n",
        "                                             number_of_pitches = Constants.number_of_pitches,\n",
        "                                             number_of_channels = Constants.number_of_channels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VgM2Popv5Sl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Validation Data Generator\n",
        "validation_data_generator = PianoRollGenerator(sample_list = validation_samples,\n",
        "                                               sampling_lower_bound_remove = sampling_lower_bound_remove,\n",
        "                                               sampling_upper_bound_remove = sampling_upper_bound_remove,\n",
        "                                               sampling_lower_bound_add = sampling_lower_bound_add,\n",
        "                                               sampling_upper_bound_add = sampling_upper_bound_add,\n",
        "                                               batch_size = batch_size, \n",
        "                                               bars = Constants.bars,\n",
        "                                               samples_per_data_item = Constants.samples_per_ground_truth_data_item,\n",
        "                                               beat_resolution = Constants.beat_resolution,\n",
        "                                               beats_per_bar = Constants.beats_per_bar, \n",
        "                                               number_of_pitches = Constants.number_of_pitches,\n",
        "                                               number_of_channels = Constants.number_of_channels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7rHxSO0v5Sn",
        "colab_type": "text"
      },
      "source": [
        "### Creating callbacks for the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6Le2G_Tv5Sn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Callback For Loss Plots \n",
        "plot_losses = GenerateTrainingPlots()\n",
        "## Checkpoint Path\n",
        "checkpoint_filepath =  'best-model-epoch:{epoch:04d}.hdf5'\n",
        "\n",
        "# Callback For Saving Model Checkpoints \n",
        "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=False,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_best_only=True)\n",
        "\n",
        "# Create A List Of Callbacks\n",
        "callbacks_list = [plot_losses, model_checkpoint_callback]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hpv3gX-Zv5Sq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create A Model Instance\n",
        "MusicModel = ArCnnModel(input_dim = input_dim,\n",
        "                        num_filters = num_filters,\n",
        "                        growth_factor = growth_factor,\n",
        "                        num_layers = num_layers,\n",
        "                        dropout_rate_encoder = dropout_rate_encoder,\n",
        "                        dropout_rate_decoder = dropout_rate_decoder,\n",
        "                        batch_norm_encoder = batch_norm_encoder,\n",
        "                        batch_norm_decoder = batch_norm_decoder,\n",
        "                        pre_trained = pre_trained,\n",
        "                        learning_rate = learning_rate,\n",
        "                        optimizer_enum = optimizer_enum)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkYdq6frv5Sr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = MusicModel.build_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-J3g35A0v5St",
        "colab_type": "text"
      },
      "source": [
        "## Starting training \n",
        "\n",
        "In the following cell, you start training your model.\n",
        "\n",
        ">**NOTE**: Training times can vary greatly based on the parameters that you have chosen and the notebook instance type that you chose when launching this notebook. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tq4RBl6uv5St",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Start Training\n",
        "history = model.fit_generator(training_data_generator,\n",
        "                              validation_data = validation_data_generator,\n",
        "                              steps_per_epoch = steps_per_epoch,\n",
        "                              epochs = epochs,\n",
        "                              callbacks = callbacks_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoJRiK-nv5Sv",
        "colab_type": "text"
      },
      "source": [
        "## Performing inference \n",
        "\n",
        "Congratulations! You have now trained your very own AR-CNN model to generate music. Now you can see how well your model will perform with an input melody. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2To_Um_av5Sv",
        "colab_type": "text"
      },
      "source": [
        "### How to change the *inference parameters* when you perform inference \n",
        "\n",
        "The model performs inference by sampling from its predicted probability distribution across the entire piano roll. \n",
        "\n",
        "Inference is an iterative process. After adding or removing a note from the input, the model feeds this new input back into itself. The model has been trained to both remove and add notes, so it can improve the input melody and correct mistakes that it may have made in earlier iterations.\n",
        "\n",
        "You also can change the *inference parameters* to observe differences in the quality of the music generated: \n",
        "\n",
        "- Sampling iterations (`samplingIterations`): The number of iterations performed during inference. A higher number of sampling iterations gives the model more time to improve the input melody.\n",
        "\n",
        "- Maximum notes to remove (`maxPercentageOfInitialNotesRemoved`): The maximum percentage of notes that can be removed during inference. Setting this value to 0% prevents the model from removing notes from your input melody.\n",
        "\n",
        "- Maximum notes to add (`maxNotesAdded`): The maximum percentage of notes that can be added during inference. Setting this value to 0% means no notes will be added to your input melody\n",
        "\n",
        ">**NOTE:** If you restrict your model's ability to add and remove notes, you risk creating poor compositions. \n",
        "\n",
        "- Creativity, `temperature`: To create the output probability distribution, the final layer uses a softmax activation. You can change the temperature for the softmax to produce different levels of creativity in the outputs generated by the model.\n",
        "\n",
        "\n",
        "#### To change the inference parameters:\n",
        "\n",
        "1. Open the `inference_parameters.json` file. \n",
        "2. Update the variables.\n",
        "3. Save and close the `inference_parameters.json` file. \n",
        "4. Run the following cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuqvqto6v5Sv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load The Inference-Related Parameters\n",
        "with open('inference_parameters.json') as json_file:\n",
        "    inference_params = json.load(json_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whYDX2pTv5Sx",
        "colab_type": "text"
      },
      "source": [
        "### Loading a saved checkpoint file\n",
        "\n",
        "To use your trained model, you will need to update the `checkpoint_var` variable in the cell below. To see the checkpoint files that you have created, uncomment and then run the following cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEMmcDmmv5Sx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls -ltr /content/aws-deepcomposer-samples/ar-cnn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mc_Sg9jv5Sz",
        "colab_type": "text"
      },
      "source": [
        "In the next code cell, replace the string in variable `checkpoint_var` with the filename, `checkpoints/foo-bar.hdf5` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clwwU8Gov5S0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content/aws-deepcomposer-samples/ar-cnn\n",
        "# Create An Inference Object\n",
        "inference_obj = Inference()\n",
        "# Load The Checkpoint\n",
        "checkpoint_var = '/content/aws-deepcomposer-samples/ar-cnn/best-model-epoch:0003.hdf5'\n",
        "inference_obj.load_model(checkpoint_var) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IfKeRMiv5S2",
        "colab_type": "text"
      },
      "source": [
        "#### To choose a new input melody\n",
        "\n",
        "1. In a different tab, switch back to the Jupyter console\n",
        "2. Open the `sample_inputs` directory. It contains six sample melodies. \n",
        "3. Note the name of the file that you want to use, for example, 'new_world.midi'. \n",
        "4. In the following cell, replace **'sample_inputs/ode_to_joy.midi'** with the name of the file and run the cell below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nt_l2Ldv5S3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate The Composition\n",
        "input_melody = '/content/aws-deepcomposer-samples/ar-cnn/unconditional-fast.mid'  \n",
        "inference_obj.generate_composition(input_melody, inference_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02OIfcixv5S5",
        "colab_type": "text"
      },
      "source": [
        "#### To listen to your composition\n",
        "\n",
        "1. Run the following cell. It lists our compositions. \n",
        "2. Note the filename of the composition that you want to listen to, for example, \"output_2.mid\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fQjvR0Iv5S5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls -ltr outputs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx-xlkhBv5S7",
        "colab_type": "text"
      },
      "source": [
        "3. In the following cell, replace **'outputs/output_0.mid'** with the name of the file and run the cell below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7m1zcvgmv5S7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output_melody = 'outputs/output_11.mid'\n",
        "play_midi(output_melody)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxnhsXr-v5S9",
        "colab_type": "text"
      },
      "source": [
        ">**NOTE**: Compositions are automatically saved."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnHjP3ZXv5S9",
        "colab_type": "text"
      },
      "source": [
        "## Evaluating your results\n",
        "\n",
        "Now that you've generated a composition, let's find out how you did by running the code cells below. The code cells below provide you with some model metrics and a visualization of the piano rolls created.    \n",
        "\n",
        "We'll analyze the composition using the following metrics: \n",
        "\n",
        "- *Empty bar rate:* The ratio of empty bars to the total number of bars. \n",
        "- *Pitch histogram distance:* The distribution and position of pitches.\n",
        "- *In scale ratio:*  The ratio of the number of notes, in the C major key, to the total number of notes. \n",
        "\n",
        "\n",
        "### Visualizing the results\n",
        "After computing the metrics, let's also visualize the *input piano roll* and compare it with the generated output piano roll to see which notes have been changed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GJJ1-B6v5S-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Input Midi Metrics:\n",
        "print(\"The input midi metrics are:\")\n",
        "get_music_metrics(input_melody, beat_resolution=4)\n",
        "\n",
        "print(\"\\n\")\n",
        "# Generated Output Midi Metrics:\n",
        "print(\"The generated output midi metrics are:\")\n",
        "get_music_metrics(output_melody, beat_resolution=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZkJhErmv5S_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert The Input and Generated Midi To Tensors (a matrix)\n",
        "input_pianoroll = process_midi(input_melody, beat_resolution=4)\n",
        "output_pianoroll = process_midi(output_melody, beat_resolution=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bx3WfMJNv5TB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot Input Piano Roll\n",
        "plot_pianoroll(input_pianoroll, beat_resolution=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQVKtmugv5TD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot Output Piano Roll\n",
        "plot_pianoroll(output_pianoroll, beat_resolution=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_rop6tPv5TE",
        "colab_type": "text"
      },
      "source": [
        "## Submitting to the *Spin the Model* Chartbusters challnege\n",
        "\n",
        "To submit your composition(s) and model to the *Spin the model* chartbusters challenge you will first need to create a public repository on [GitHub](https://github.com/). Then download your notebook, checkpoint files, and compositions from SageMaker, and upload them to your public repository. Use the link from your public repository to make your submission to the Chartbusters challenge! "
      ]
    }
  ]
}